starting org.apache.spark.deploy.master.Master, logging to /storage/work/tkk5297/Lab7/spark-tkk5297-org.apache.spark.deploy.master.Master-1-p-bc-5039.out
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
24/10/11 12:50:44 INFO Worker: Started daemon with process name: 2433399@p-bc-5039
24/10/11 12:50:44 INFO SignalUtils: Registering signal handler for TERM
24/10/11 12:50:44 INFO SignalUtils: Registering signal handler for HUP
24/10/11 12:50:44 INFO SignalUtils: Registering signal handler for INT
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
24/10/11 12:50:44 INFO Worker: Started daemon with process name: 1656465@p-bc-5046
24/10/11 12:50:44 INFO SignalUtils: Registering signal handler for TERM
24/10/11 12:50:44 INFO SignalUtils: Registering signal handler for HUP
24/10/11 12:50:44 INFO SignalUtils: Registering signal handler for INT
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
24/10/11 12:50:44 INFO Worker: Started daemon with process name: 3500618@p-bc-5054
24/10/11 12:50:44 INFO SignalUtils: Registering signal handler for TERM
24/10/11 12:50:44 INFO SignalUtils: Registering signal handler for HUP
24/10/11 12:50:44 INFO SignalUtils: Registering signal handler for INT
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
24/10/11 12:50:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/10/11 12:50:44 INFO Worker: Started daemon with process name: 3829014@p-bc-5044
24/10/11 12:50:44 INFO SignalUtils: Registering signal handler for TERM
24/10/11 12:50:44 INFO SignalUtils: Registering signal handler for HUP
24/10/11 12:50:44 INFO SignalUtils: Registering signal handler for INT
24/10/11 12:50:44 INFO SecurityManager: Changing view acls to: tkk5297
24/10/11 12:50:44 INFO SecurityManager: Changing modify acls to: tkk5297
24/10/11 12:50:44 INFO SecurityManager: Changing view acls groups to: 
24/10/11 12:50:44 INFO SecurityManager: Changing modify acls groups to: 
24/10/11 12:50:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tkk5297); groups with view permissions: Set(); users  with modify permissions: Set(tkk5297); groups with modify permissions: Set()
24/10/11 12:50:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/10/11 12:50:45 INFO SecurityManager: Changing view acls to: tkk5297
24/10/11 12:50:45 INFO SecurityManager: Changing modify acls to: tkk5297
24/10/11 12:50:45 INFO SecurityManager: Changing view acls groups to: 
24/10/11 12:50:45 INFO SecurityManager: Changing modify acls groups to: 
24/10/11 12:50:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tkk5297); groups with view permissions: Set(); users  with modify permissions: Set(tkk5297); groups with modify permissions: Set()
24/10/11 12:50:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/10/11 12:50:45 INFO Utils: Successfully started service 'sparkWorker' on port 41921.
24/10/11 12:50:45 INFO Worker: Worker decommissioning not enabled.
24/10/11 12:50:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/10/11 12:50:45 INFO SecurityManager: Changing view acls to: tkk5297
24/10/11 12:50:45 INFO SecurityManager: Changing modify acls to: tkk5297
24/10/11 12:50:45 INFO SecurityManager: Changing view acls groups to: 
24/10/11 12:50:45 INFO SecurityManager: Changing modify acls groups to: 
24/10/11 12:50:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tkk5297); groups with view permissions: Set(); users  with modify permissions: Set(tkk5297); groups with modify permissions: Set()
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
24/10/11 12:50:45 INFO SecurityManager: Changing view acls to: tkk5297
24/10/11 12:50:45 INFO SecurityManager: Changing modify acls to: tkk5297
24/10/11 12:50:45 INFO SecurityManager: Changing view acls groups to: 
24/10/11 12:50:45 INFO SecurityManager: Changing modify acls groups to: 
24/10/11 12:50:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tkk5297); groups with view permissions: Set(); users  with modify permissions: Set(tkk5297); groups with modify permissions: Set()
24/10/11 12:50:45 INFO Utils: Successfully started service 'sparkWorker' on port 43607.
24/10/11 12:50:45 INFO Worker: Worker decommissioning not enabled.
24/10/11 12:50:45 INFO Worker: Started daemon with process name: 3036335@p-bc-5043
24/10/11 12:50:45 INFO SignalUtils: Registering signal handler for TERM
24/10/11 12:50:45 INFO SignalUtils: Registering signal handler for HUP
24/10/11 12:50:45 INFO SignalUtils: Registering signal handler for INT
24/10/11 12:50:45 INFO Worker: Starting Spark worker 10.6.8.49:41921 with 4 cores, 15.0 GiB RAM
24/10/11 12:50:45 INFO Worker: Running Spark version 3.3.0
24/10/11 12:50:45 INFO Worker: Spark home: /storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3
24/10/11 12:50:45 INFO ResourceUtils: ==============================================================
24/10/11 12:50:45 INFO ResourceUtils: No custom resources configured for spark.worker.
24/10/11 12:50:45 INFO ResourceUtils: ==============================================================
24/10/11 12:50:45 INFO Worker: Starting Spark worker 10.6.8.56:43607 with 4 cores, 15.0 GiB RAM
24/10/11 12:50:45 INFO Worker: Running Spark version 3.3.0
24/10/11 12:50:45 INFO Worker: Spark home: /storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3
24/10/11 12:50:45 INFO Utils: Successfully started service 'sparkWorker' on port 44519.
24/10/11 12:50:45 INFO Worker: Worker decommissioning not enabled.
24/10/11 12:50:45 INFO ResourceUtils: ==============================================================
24/10/11 12:50:45 INFO ResourceUtils: No custom resources configured for spark.worker.
24/10/11 12:50:45 INFO ResourceUtils: ==============================================================
24/10/11 12:50:45 WARN Utils: Service 'WorkerUI' could not bind on port 8081. Attempting port 8082.
24/10/11 12:50:45 WARN Utils: Service 'WorkerUI' could not bind on port 8082. Attempting port 8083.
24/10/11 12:50:45 INFO Utils: Successfully started service 'WorkerUI' on port 8083.
24/10/11 12:50:45 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://p-bc-5039.2e.hpc.psu.edu:8083
24/10/11 12:50:45 INFO Worker: Connecting to master p-bc-5039:7077...
24/10/11 12:50:45 INFO Utils: Successfully started service 'sparkWorker' on port 33077.
24/10/11 12:50:45 INFO Worker: Worker decommissioning not enabled.
24/10/11 12:50:45 WARN Utils: Service 'WorkerUI' could not bind on port 8081. Attempting port 8082.
24/10/11 12:50:45 INFO TransportClientFactory: Successfully created connection to p-bc-5039/10.6.8.49:7077 after 42 ms (0 ms spent in bootstraps)
24/10/11 12:50:45 INFO Utils: Successfully started service 'WorkerUI' on port 8082.
24/10/11 12:50:45 INFO Worker: Starting Spark worker 10.6.8.64:44519 with 4 cores, 15.0 GiB RAM
24/10/11 12:50:45 INFO Worker: Running Spark version 3.3.0
24/10/11 12:50:45 INFO Worker: Spark home: /storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3
24/10/11 12:50:45 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://p-bc-5046.2e.hpc.psu.edu:8082
24/10/11 12:50:45 INFO Worker: Connecting to master p-bc-5039:7077...
24/10/11 12:50:45 INFO ResourceUtils: ==============================================================
24/10/11 12:50:45 INFO ResourceUtils: No custom resources configured for spark.worker.
24/10/11 12:50:45 INFO ResourceUtils: ==============================================================
24/10/11 12:50:45 INFO TransportClientFactory: Successfully created connection to p-bc-5039/10.6.8.49:7077 after 32 ms (0 ms spent in bootstraps)
24/10/11 12:50:45 INFO Worker: Successfully registered with master spark://p-bc-5039:7077
24/10/11 12:50:45 INFO Worker: Starting Spark worker 10.6.8.54:33077 with 4 cores, 15.0 GiB RAM
24/10/11 12:50:45 INFO Worker: Running Spark version 3.3.0
24/10/11 12:50:45 INFO Worker: Spark home: /storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3
24/10/11 12:50:45 INFO Worker: Successfully registered with master spark://p-bc-5039:7077
24/10/11 12:50:45 INFO ResourceUtils: ==============================================================
24/10/11 12:50:45 INFO ResourceUtils: No custom resources configured for spark.worker.
24/10/11 12:50:45 INFO ResourceUtils: ==============================================================
24/10/11 12:50:45 WARN Utils: Service 'WorkerUI' could not bind on port 8081. Attempting port 8082.
24/10/11 12:50:45 INFO Utils: Successfully started service 'WorkerUI' on port 8082.
24/10/11 12:50:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/10/11 12:50:46 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://p-bc-5054.2e.hpc.psu.edu:8082
24/10/11 12:50:46 INFO Worker: Connecting to master p-bc-5039:7077...
24/10/11 12:50:46 INFO TransportClientFactory: Successfully created connection to p-bc-5039/10.6.8.49:7077 after 41 ms (0 ms spent in bootstraps)
24/10/11 12:50:46 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
24/10/11 12:50:46 INFO SecurityManager: Changing view acls to: tkk5297
24/10/11 12:50:46 INFO SecurityManager: Changing modify acls to: tkk5297
24/10/11 12:50:46 INFO SecurityManager: Changing view acls groups to: 
24/10/11 12:50:46 INFO SecurityManager: Changing modify acls groups to: 
24/10/11 12:50:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tkk5297); groups with view permissions: Set(); users  with modify permissions: Set(tkk5297); groups with modify permissions: Set()
24/10/11 12:50:46 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://p-bc-5044.2e.hpc.psu.edu:8081
24/10/11 12:50:46 INFO Worker: Connecting to master p-bc-5039:7077...
24/10/11 12:50:46 INFO Worker: Successfully registered with master spark://p-bc-5039:7077
24/10/11 12:50:46 INFO TransportClientFactory: Successfully created connection to p-bc-5039/10.6.8.49:7077 after 39 ms (0 ms spent in bootstraps)
24/10/11 12:50:46 INFO Worker: Successfully registered with master spark://p-bc-5039:7077
24/10/11 12:50:46 INFO Utils: Successfully started service 'sparkWorker' on port 35619.
24/10/11 12:50:46 INFO Worker: Worker decommissioning not enabled.
24/10/11 12:50:47 INFO Worker: Starting Spark worker 10.6.8.53:35619 with 4 cores, 15.0 GiB RAM
24/10/11 12:50:47 INFO Worker: Running Spark version 3.3.0
24/10/11 12:50:47 INFO Worker: Spark home: /storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3
24/10/11 12:50:47 INFO ResourceUtils: ==============================================================
24/10/11 12:50:47 INFO ResourceUtils: No custom resources configured for spark.worker.
24/10/11 12:50:47 INFO ResourceUtils: ==============================================================
24/10/11 12:50:47 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
24/10/11 12:50:47 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://p-bc-5043.2e.hpc.psu.edu:8081
24/10/11 12:50:47 INFO Worker: Connecting to master p-bc-5039:7077...
24/10/11 12:50:47 INFO TransportClientFactory: Successfully created connection to p-bc-5039/10.6.8.49:7077 after 90 ms (0 ms spent in bootstraps)
24/10/11 12:50:47 INFO Worker: Successfully registered with master spark://p-bc-5039:7077
24/10/11 12:51:05 INFO SparkContext: Running Spark version 3.3.0
24/10/11 12:51:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
24/10/11 12:51:05 INFO ResourceUtils: ==============================================================
24/10/11 12:51:05 INFO ResourceUtils: No custom resources configured for spark.driver.
24/10/11 12:51:05 INFO ResourceUtils: ==============================================================
24/10/11 12:51:05 INFO SparkContext: Submitted application: Lab7 Persist and BMB hastag changes
24/10/11 12:51:05 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/10/11 12:51:05 INFO ResourceProfile: Limiting resource is cpu
24/10/11 12:51:05 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/10/11 12:51:06 INFO SecurityManager: Changing view acls to: tkk5297
24/10/11 12:51:06 INFO SecurityManager: Changing modify acls to: tkk5297
24/10/11 12:51:06 INFO SecurityManager: Changing view acls groups to: 
24/10/11 12:51:06 INFO SecurityManager: Changing modify acls groups to: 
24/10/11 12:51:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tkk5297); groups with view permissions: Set(); users  with modify permissions: Set(tkk5297); groups with modify permissions: Set()
24/10/11 12:51:06 INFO Utils: Successfully started service 'sparkDriver' on port 43575.
24/10/11 12:51:06 INFO SparkEnv: Registering MapOutputTracker
24/10/11 12:51:06 INFO SparkEnv: Registering BlockManagerMaster
24/10/11 12:51:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/10/11 12:51:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/10/11 12:51:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/10/11 12:51:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cac118bf-56cf-4e8c-a8a1-388390621750
24/10/11 12:51:06 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/10/11 12:51:06 INFO SparkEnv: Registering OutputCommitCoordinator
24/10/11 12:51:06 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/10/11 12:51:06 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://p-bc-5039:7077...
24/10/11 12:51:06 INFO TransportClientFactory: Successfully created connection to p-bc-5039/10.6.8.49:7077 after 27 ms (0 ms spent in bootstraps)
24/10/11 12:51:07 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20241011125107-0000
24/10/11 12:51:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45187.
24/10/11 12:51:07 INFO NettyBlockTransferService: Server created on p-bc-5039.2e.hpc.psu.edu:45187
24/10/11 12:51:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/10/11 12:51:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, p-bc-5039.2e.hpc.psu.edu, 45187, None)
24/10/11 12:51:07 INFO BlockManagerMasterEndpoint: Registering block manager p-bc-5039.2e.hpc.psu.edu:45187 with 434.4 MiB RAM, BlockManagerId(driver, p-bc-5039.2e.hpc.psu.edu, 45187, None)
24/10/11 12:51:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, p-bc-5039.2e.hpc.psu.edu, 45187, None)
24/10/11 12:51:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, p-bc-5039.2e.hpc.psu.edu, 45187, None)
24/10/11 12:51:07 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241011125107-0000/0 on worker-20241011125045-10.6.8.64-44519 (10.6.8.64:44519) with 4 core(s)
24/10/11 12:51:07 INFO StandaloneSchedulerBackend: Granted executor ID app-20241011125107-0000/0 on hostPort 10.6.8.64:44519 with 4 core(s), 1024.0 MiB RAM
24/10/11 12:51:07 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241011125107-0000/1 on worker-20241011125046-10.6.8.53-35619 (10.6.8.53:35619) with 4 core(s)
24/10/11 12:51:07 INFO StandaloneSchedulerBackend: Granted executor ID app-20241011125107-0000/1 on hostPort 10.6.8.53:35619 with 4 core(s), 1024.0 MiB RAM
24/10/11 12:51:07 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241011125107-0000/2 on worker-20241011125045-10.6.8.56-43607 (10.6.8.56:43607) with 4 core(s)
24/10/11 12:51:07 INFO StandaloneSchedulerBackend: Granted executor ID app-20241011125107-0000/2 on hostPort 10.6.8.56:43607 with 4 core(s), 1024.0 MiB RAM
24/10/11 12:51:07 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241011125107-0000/3 on worker-20241011125045-10.6.8.54-33077 (10.6.8.54:33077) with 4 core(s)
24/10/11 12:51:07 INFO StandaloneSchedulerBackend: Granted executor ID app-20241011125107-0000/3 on hostPort 10.6.8.54:33077 with 4 core(s), 1024.0 MiB RAM
24/10/11 12:51:07 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20241011125107-0000/4 on worker-20241011125045-10.6.8.49-41921 (10.6.8.49:41921) with 4 core(s)
24/10/11 12:51:07 INFO StandaloneSchedulerBackend: Granted executor ID app-20241011125107-0000/4 on hostPort 10.6.8.49:41921 with 4 core(s), 1024.0 MiB RAM
24/10/11 12:51:07 INFO Worker: Asked to launch executor app-20241011125107-0000/2 for Lab7 Persist and BMB hastag changes
24/10/11 12:51:07 INFO Worker: Asked to launch executor app-20241011125107-0000/4 for Lab7 Persist and BMB hastag changes
24/10/11 12:51:07 INFO Worker: Asked to launch executor app-20241011125107-0000/0 for Lab7 Persist and BMB hastag changes
24/10/11 12:51:07 INFO Worker: Asked to launch executor app-20241011125107-0000/3 for Lab7 Persist and BMB hastag changes
24/10/11 12:51:07 INFO SecurityManager: Changing view acls to: tkk5297
24/10/11 12:51:07 INFO SecurityManager: Changing modify acls to: tkk5297
24/10/11 12:51:07 INFO SecurityManager: Changing view acls groups to: 
24/10/11 12:51:07 INFO SecurityManager: Changing modify acls groups to: 
24/10/11 12:51:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tkk5297); groups with view permissions: Set(); users  with modify permissions: Set(tkk5297); groups with modify permissions: Set()
24/10/11 12:51:07 INFO SecurityManager: Changing view acls to: tkk5297
24/10/11 12:51:07 INFO SecurityManager: Changing modify acls to: tkk5297
24/10/11 12:51:07 INFO SecurityManager: Changing view acls groups to: 
24/10/11 12:51:07 INFO SecurityManager: Changing modify acls groups to: 
24/10/11 12:51:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tkk5297); groups with view permissions: Set(); users  with modify permissions: Set(tkk5297); groups with modify permissions: Set()
24/10/11 12:51:07 INFO SecurityManager: Changing view acls to: tkk5297
24/10/11 12:51:07 INFO SecurityManager: Changing modify acls to: tkk5297
24/10/11 12:51:07 INFO SecurityManager: Changing view acls groups to: 
24/10/11 12:51:07 INFO SecurityManager: Changing modify acls groups to: 
24/10/11 12:51:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tkk5297); groups with view permissions: Set(); users  with modify permissions: Set(tkk5297); groups with modify permissions: Set()
24/10/11 12:51:07 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-11-openjdk-11.0.24.0.8-3.el8.x86_64/bin/java" "-cp" "/storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3/conf/:/storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3/jars/*" "-Xmx1024M" "-Dspark.driver.port=43575" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@p-bc-5039.2e.hpc.psu.edu:43575" "--executor-id" "2" "--hostname" "10.6.8.56" "--cores" "4" "--app-id" "app-20241011125107-0000" "--worker-url" "spark://Worker@10.6.8.56:43607"
24/10/11 12:51:07 INFO Worker: Asked to launch executor app-20241011125107-0000/1 for Lab7 Persist and BMB hastag changes
24/10/11 12:51:07 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-11-openjdk-11.0.24.0.8-3.el8.x86_64/bin/java" "-cp" "/storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3/conf/:/storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3/jars/*" "-Xmx1024M" "-Dspark.driver.port=43575" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@p-bc-5039.2e.hpc.psu.edu:43575" "--executor-id" "0" "--hostname" "10.6.8.64" "--cores" "4" "--app-id" "app-20241011125107-0000" "--worker-url" "spark://Worker@10.6.8.64:44519"
24/10/11 12:51:07 INFO SecurityManager: Changing view acls to: tkk5297
24/10/11 12:51:07 INFO SecurityManager: Changing modify acls to: tkk5297
24/10/11 12:51:07 INFO SecurityManager: Changing view acls groups to: 
24/10/11 12:51:07 INFO SecurityManager: Changing modify acls groups to: 
24/10/11 12:51:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tkk5297); groups with view permissions: Set(); users  with modify permissions: Set(tkk5297); groups with modify permissions: Set()
24/10/11 12:51:07 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-11-openjdk-11.0.24.0.8-3.el8.x86_64/bin/java" "-cp" "/storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3/conf/:/storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3/jars/*" "-Xmx1024M" "-Dspark.driver.port=43575" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@p-bc-5039.2e.hpc.psu.edu:43575" "--executor-id" "4" "--hostname" "10.6.8.49" "--cores" "4" "--app-id" "app-20241011125107-0000" "--worker-url" "spark://Worker@10.6.8.49:41921"
24/10/11 12:51:07 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-11-openjdk-11.0.24.0.8-3.el8.x86_64/bin/java" "-cp" "/storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3/conf/:/storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3/jars/*" "-Xmx1024M" "-Dspark.driver.port=43575" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@p-bc-5039.2e.hpc.psu.edu:43575" "--executor-id" "3" "--hostname" "10.6.8.54" "--cores" "4" "--app-id" "app-20241011125107-0000" "--worker-url" "spark://Worker@10.6.8.54:33077"
24/10/11 12:51:07 INFO SecurityManager: Changing view acls to: tkk5297
24/10/11 12:51:07 INFO SecurityManager: Changing modify acls to: tkk5297
24/10/11 12:51:07 INFO SecurityManager: Changing view acls groups to: 
24/10/11 12:51:07 INFO SecurityManager: Changing modify acls groups to: 
24/10/11 12:51:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(tkk5297); groups with view permissions: Set(); users  with modify permissions: Set(tkk5297); groups with modify permissions: Set()
24/10/11 12:51:07 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241011125107-0000/2 is now RUNNING
24/10/11 12:51:07 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241011125107-0000/0 is now RUNNING
24/10/11 12:51:07 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241011125107-0000/3 is now RUNNING
24/10/11 12:51:07 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241011125107-0000/4 is now RUNNING
24/10/11 12:51:07 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-11-openjdk-11.0.24.0.8-3.el8.x86_64/bin/java" "-cp" "/storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3/conf/:/storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3/jars/*" "-Xmx1024M" "-Dspark.driver.port=43575" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@p-bc-5039.2e.hpc.psu.edu:43575" "--executor-id" "1" "--hostname" "10.6.8.53" "--cores" "4" "--app-id" "app-20241011125107-0000" "--worker-url" "spark://Worker@10.6.8.53:35619"
24/10/11 12:51:07 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
24/10/11 12:51:07 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20241011125107-0000/1 is now RUNNING
24/10/11 12:51:30 INFO Worker: Asked to kill executor app-20241011125107-0000/0
24/10/11 12:51:30 INFO ExecutorRunner: Runner thread for executor app-20241011125107-0000/0 interrupted
24/10/11 12:51:30 INFO Worker: Asked to kill executor app-20241011125107-0000/4
24/10/11 12:51:30 INFO ExecutorRunner: Killing process!
24/10/11 12:51:30 INFO ExecutorRunner: Runner thread for executor app-20241011125107-0000/4 interrupted
24/10/11 12:51:30 INFO Worker: Asked to kill executor app-20241011125107-0000/2
24/10/11 12:51:30 INFO Worker: Asked to kill executor app-20241011125107-0000/3
24/10/11 12:51:30 INFO ExecutorRunner: Killing process!
24/10/11 12:51:30 INFO ExecutorRunner: Runner thread for executor app-20241011125107-0000/2 interrupted
24/10/11 12:51:30 INFO ExecutorRunner: Killing process!
24/10/11 12:51:30 INFO Worker: Asked to kill executor app-20241011125107-0000/1
24/10/11 12:51:30 INFO ExecutorRunner: Runner thread for executor app-20241011125107-0000/3 interrupted
24/10/11 12:51:30 INFO ExecutorRunner: Killing process!
24/10/11 12:51:30 INFO ExecutorRunner: Runner thread for executor app-20241011125107-0000/1 interrupted
24/10/11 12:51:30 INFO ExecutorRunner: Killing process!
24/10/11 12:51:30 INFO Worker: Executor app-20241011125107-0000/0 finished with state KILLED exitStatus 143
24/10/11 12:51:30 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
24/10/11 12:51:30 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241011125107-0000, execId=0)
24/10/11 12:51:30 INFO ExternalShuffleBlockResolver: Application app-20241011125107-0000 removed, cleanupLocalDirs = true
24/10/11 12:51:30 INFO Worker: Cleaning up local directories for application app-20241011125107-0000
24/10/11 12:51:30 INFO Worker: Executor app-20241011125107-0000/2 finished with state KILLED exitStatus 0
24/10/11 12:51:30 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 2
24/10/11 12:51:30 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241011125107-0000, execId=2)
24/10/11 12:51:30 INFO Worker: Executor app-20241011125107-0000/4 finished with state KILLED exitStatus 0
24/10/11 12:51:30 INFO ExternalShuffleBlockResolver: Application app-20241011125107-0000 removed, cleanupLocalDirs = true
24/10/11 12:51:30 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 4
24/10/11 12:51:30 INFO Worker: Cleaning up local directories for application app-20241011125107-0000
24/10/11 12:51:30 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241011125107-0000, execId=4)
24/10/11 12:51:30 INFO ExternalShuffleBlockResolver: Application app-20241011125107-0000 removed, cleanupLocalDirs = true
24/10/11 12:51:30 INFO Worker: Cleaning up local directories for application app-20241011125107-0000
24/10/11 12:51:30 INFO Worker: Executor app-20241011125107-0000/1 finished with state KILLED exitStatus 143
24/10/11 12:51:30 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 1
24/10/11 12:51:30 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241011125107-0000, execId=1)
24/10/11 12:51:30 INFO ExternalShuffleBlockResolver: Application app-20241011125107-0000 removed, cleanupLocalDirs = true
24/10/11 12:51:30 INFO Worker: Cleaning up local directories for application app-20241011125107-0000
24/10/11 12:51:30 INFO Worker: Executor app-20241011125107-0000/3 finished with state KILLED exitStatus 143
24/10/11 12:51:30 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 3
24/10/11 12:51:30 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20241011125107-0000, execId=3)
24/10/11 12:51:30 INFO ExternalShuffleBlockResolver: Application app-20241011125107-0000 removed, cleanupLocalDirs = true
24/10/11 12:51:30 INFO Worker: Cleaning up local directories for application app-20241011125107-0000
/storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3/sbin/start-master.sh
/storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3/bin/spark-class org.apache.spark.deploy.worker.Worker --work-dir /storage/work/tkk5297/Lab7 spark://p-bc-5039:7077
SPARK_MASTER_HOST=p-bc-5039
SPARK_MASTER_PORT=7077

real	0m54.492s
user	0m19.290s
sys	0m1.222s
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 23672620.0 ON p-bc-5039 CANCELLED AT 2024-10-11T13:47:51 DUE TO TIME LIMIT ***
24/10/11 13:47:51 ERROR Worker: RECEIVED SIGNAL TERM
24/10/11 13:47:51 ERROR Worker: RECEIVED SIGNAL TERM
24/10/11 13:47:51 ERROR Worker: RECEIVED SIGNAL TERM
24/10/11 13:47:51 ERROR Worker: RECEIVED SIGNAL TERM
24/10/11 13:47:51 INFO ShutdownHookManager: Shutdown hook called
24/10/11 13:47:51 ERROR Worker: RECEIVED SIGNAL TERM
24/10/11 13:47:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-7b318bde-6fdc-4477-9e02-e7301feb106a
24/10/11 13:47:51 INFO ShutdownHookManager: Shutdown hook called
24/10/11 13:47:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-53061996-ab9a-4382-8b63-2ab6f25cc1a8
24/10/11 13:47:51 INFO ShutdownHookManager: Shutdown hook called
24/10/11 13:47:51 INFO ShutdownHookManager: Shutdown hook called
24/10/11 13:47:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-13b80c77-74bc-43a0-bbd8-fff2bbfa01b2
24/10/11 13:47:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-b69b3fbc-2b56-4f97-b98f-01da6c23a5b6
24/10/11 13:47:51 INFO ShutdownHookManager: Shutdown hook called
24/10/11 13:47:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-43feb9f6-9082-441c-89de-0ceb4a1e8a8a
srun: error: p-bc-5039: task 0: Exited with exit code 143
srun: error: p-bc-5054: task 4: Exited with exit code 143
srun: error: p-bc-5046: task 3: Exited with exit code 143
srun: error: p-bc-5044: task 2: Exited with exit code 143
srun: error: p-bc-5043: task 1: Exited with exit code 143
