{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS/CMPSC 410 Fall 2024\n",
    "## Instructor: Professor John Yen\n",
    "## TA: Jin Peng and Al Lawati, Ali Hussain Mohsi\n",
    "\n",
    "## Lab 7 Persist and Performance \n",
    "## The goals of this lab are for you to be able to\n",
    "## - Choose the RDD for applying persist.\n",
    "## - Apply the obove to compute hastag counts of Tweets after Boston Marathon Bombing (April 19 and April 20).\n",
    "##  This lab includes four data sets, as explained below.\n",
    "## Data\n",
    "- The first dataset contains Boston Marathon Bombing collected on 4/19/2013.\n",
    "- The second dataset contains Boston Marathon Bombing collected on 4/20/2013.\n",
    "- To facilitate this analytics task both in the local mode and in the cluster mode, a smaller sampled dataset for each day's tweets has also been provided: sampled_4_19_tweets.csv and sampled_4_20_tweets.csv.  You should use these two data sets for running Spark in the local mode (using Jupyter Notebook).  However, you should change input data for spark-submit (cluster mode) to the big dataset for each day: BMB_4_19_tweets.csv and BMB_4_20_tweets.csv.\n",
    "- You should create a Lab7 subdirectory under work, because more space is available under work directory. Download both datasets to your directory `<home>/work/Lab7/`.\n",
    "- You should also download the Jupyter Notebook for Lab 7 to the same directory.\n",
    "## Problem\n",
    "- The problem we want to solve is to (1) find hashtags that in 4/19/2013 tweets and in 4/20/2013 tweets, and (2) calculate the difference of total hashtag counts of these hashtags (considering both days). You should save these hashtags together with their counts in a text file.\n",
    "## Two Step\n",
    "- You will first run in Spark local mode (and Jupyter Notebook) for the small sample for each day.\n",
    "- After you obtain the result for small datasets, you can then convert the code for local mode into code for cluster mode, and submit the code to ICDS cluster and obtain run-time performance.  \n",
    "1. Export the Jupyter Notebook as Lab7.py file.\n",
    "2. Remove ``master=\"local\",\" from SparkContext.\n",
    "3. Modify the Lab7.py file so that it reads from BMB_4_19_tweets.csv and BMB_4_20_tweets.csv.\n",
    "4. Modify Lab7.py so that outputs are saved in directories different from those used in the local model\n",
    "5. Follow step 4 and step 5 of the instructions in \"Running Spark in Cluster Mode v3 using salloc\" (Lab 5). \n",
    "\n",
    "## Submit the following items for Lab 7\n",
    "- Completed Jupyter Notebook of Lab 7 (15%)\n",
    "- Lab4C.py (used for spark-submit) (15%)\n",
    "- Log file for Lab7.py             (15%)\n",
    "- The output file of hashtag count difference generated by running spark-submit (cluster mode) on Lab7.py. (15%)\n",
    "- Lab7P.py (adding persist() to RDD's of your choice) (15%)\n",
    "- Log file for Lab7P.py  (15%)\n",
    "\n",
    "## Total Points: 90 points\n",
    "\n",
    "# Due: midnight, Oct 13th (Sunday).\n",
    "# Early Submission Bonus (10 points): midnight Friday (Oct 11th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Because we are not using DataFrame in this lab, we will be creating SparkContext rather than SparkSession\n",
    "\n",
    "- Note: We use \"local\" as the master parameter for ``SparkContext`` in this notebook so that we can run and debug it in ICDS Jupyter Server.  However, we need to remove ``\"master=\"local\",``later when you convert this notebook into a .py file for running it in the cluster mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=SparkContext(master=\"local\", appName=\"Lab7 Persist and BMB hastag changes\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 (5 points)  Add your name below \n",
    "## Answer for Exercise 1\n",
    "- Your Name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing hastag count difference includes three steps:\n",
    "- Step 1: Compute (and save) hashtag counts for April 19th tweets. \n",
    "- Step 2: Compute (and save) hashtag counts for April 20th tweets. \n",
    "- Step 3: Combine the hashtag counts of two days, compute their difference, sort based on the difference. Save the hashtag count difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Compute (and save) hashtag counts for April 19th tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete the path and run the code below to read the file \"sampled_4_19_BMBtweets.csv\" from your Lab7 directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_D1_RDD = sc.textFile(\"/storage/home/juy1/work/Lab7/sampled_4_19_BMBtweets.csv\")\n",
    "tweets_D1_RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Execute the code below, which computes the total count of hashtags in the input tweets, sort them by count (in descending order), and save them in an output directory:\n",
    "- (a) Uses flatMap to \"flatten\" the list of tokens from each tweet (using split function) into a very large list of tokens.\n",
    "- (b) Filter the token for hashtags.\n",
    "- (c) Count the total number of hashtags in a way similar to Lab 2.\n",
    "- (d) Sort the hashtag count in descending order.\n",
    "- (e) Save the sorted hashtag counts in an output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_D1_RDD = tweets_D1_RDD.flatMap(lambda line: line.strip().split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_D1_RDD = tokens_D1_RDD.filter(lambda x: x.startswith(\"#\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_1_D1_RDD = hashtag_D1_RDD.map(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_count_D1_RDD = hashtag_1_D1_RDD.reduceByKey(lambda x, y: x+y , 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_hashtag_count_D1_RDD = hashtag_count_D1_RDD.sortBy(lambda pair: pair[1] , ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: You need to complete the path with your output directory. \n",
    "### Note: You also need to change the directory names (e.g., replace \"_local.txt\" with \"_cluster.txt\") in your .py file for running Spark in cluster mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path1 = \"/storage/home/juy1/work/Lab7/sorted_BMB_hashtag_count_4_19_local.txt\" \n",
    "sorted_hashtag_count_D1_RDD.saveAsTextFile(output_path1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 Compute and save hashtag counts for April 20th tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_D2_RDD = sc.textFile(\"/storage/home/juy1/work/Lab7/sampled_4_20_BMBtweets.csv\")\n",
    "tweets_D2_RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below computes the total count of hashtags in the input tweets, sort them by count (in descending order), and save them in an output directory:\n",
    "- (a) Uses flatMap to \"flatten\" the list of tokens from each tweet (using split function) into a very large list of tokens.\n",
    "- (b) Filter the token for hashtags.\n",
    "- (c) Count the total number of hashtags in a way similar to Lab 2.\n",
    "- (d) Sort the hashtag count in descending order.\n",
    "- (e) Save the sorted hashtag counts in an output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_D2_RDD = tweets_D2_RDD.flatMap(lambda line: line.strip().split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_D2_RDD = tokens_D2_RDD.filter(lambda x: x.startswith(\"#\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_1_D2_RDD = hashtag_D2_RDD.map(lambda x: (x, 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_count_D2_RDD = hashtag_1_D2_RDD.reduceByKey(lambda x, y : x+y, 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_hashtag_count_D2_RDD = hashtag_count_D2_RDD.sortBy(lambda pair: pair[1], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: You need to complete the path with your output directory. \n",
    "### Note: You also need to change the directory names (e.g., replace \"_sampled\" with \"_cluster\") before you convert this notebook into a .py file for submiting it to ICDS cluster.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path2 = \"/storage/home/juy1/work/Lab7/sorted_BMB_hashtag_count_4_20_local.txt\" \n",
    "sorted_hashtag_count_D2_RDD.saveAsTextFile(output_path2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Combine the hashcount of two days, compute their difference, and save sorted difference of hashtag counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below joins the two hash_count key value pairs RDDs on hashtag (i.e., key), and  compute the difference of the hashtag counts between the two days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_count_D1_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_count_D2_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_hashtag_count_RDD = hashtag_count_D1_RDD.fullOuterJoin(hashtag_count_D2_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_hashtag_count_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none1_RDD = joined_hashtag_count_RDD.filter(lambda pair: pair[1][0]==None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "none1_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tran_none(x):\n",
    "    if (x==None) :\n",
    "        return(0)\n",
    "    else:\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_count_diff_RDD = joined_hashtag_count_RDD.map(lambda x: (x[0], tran_none(x[1][1])-tran_none(x[1][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_count_diff_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_hashtag_count_diff_RDD = hashtag_count_diff_RDD.sortBy(lambda x: x[1], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_hashtag_count_diff_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path3 = \"/storage/home/juy1/work/Lab7/sorted_BMB_hashtag_diff_local.txt\" \n",
    "sorted_hashtag_count_diff_RDD.saveAsTextFile(output_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
