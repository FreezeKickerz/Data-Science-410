{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS/CMPSC 410 Fall 2024\n",
    "## Instructor: Professor John Yen\n",
    "## \n",
    "# Lab 2: MapReduce in Spark\n",
    "\n",
    "## The goals of this lab are for you to be able to\n",
    "## - Implement MapReduce using map, flatMap, and reduceByKey in Spark\n",
    "## - Apply the above for calculating total counts of words in a textfile.\n",
    "## - Be able to use .take to show contents of an RDD.\n",
    "## - Be able to generate an error message due to a typo in the input file.\n",
    "## Total Number of Exercises: \n",
    "- Exercise 1: 5 points\n",
    "- Exercise 2: 5 points\n",
    "- Exercise 3: 5 points\n",
    "- Exercise 4: 10 points\n",
    "- Exercise 5: 10 points\n",
    "- Exercise 6: 10 points\n",
    "- Exercise 7: 5 points\n",
    "## Total Points: 50 points\n",
    "\n",
    "# Due: midnight, September 8, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The first thing we need to do in each Jupyter Notebook running pyspark is to import pyspark first.\n",
    "# Note: You should have followed the instruction of Lab 2 (word document) to install PySpark in your ICDS account.  If not, complete all steps in the instructions before you proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once we import pyspark, we need to import an important object called \"SparkContext\".  \n",
    "- Note: Every spark program needs a SparkContext object, which provides critical information (e.g., whether this Spark session runs in local mode or cluster mode) for the run-time environment of the Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We then create a Spark Context variable.  Once we have a spark context variable, we can execute spark codes. \n",
    "- The first parameter specifies that this Spark session is running in \"local\" mode. \n",
    "- The second parameter specifies a name for your Spark application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In creating the Spark Context variable below, we specified that this spark code is running in a \n",
    "`local`\n",
    "### mode, with a name\n",
    "`Lab2`.\n",
    "### After you run the cell below, be patient to wait for its completion before you \"run\" the next cell. When the left of the cell shows \n",
    "`[*]:`, \n",
    "### it means the 'run' is not completed yet. The completion of running a cell is indicated by a number in the brackets such as\n",
    "`[3]:`.\n",
    "### Note: You MUST wait for a cell to complete before you 'run' the next cell in PySpark Jupyter notebook. Otherwise, the execution of your later cells may generate errors because the inputs they need are not available yet (e.g., been not been generated by a previous PySpark statement that is still running)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=SparkContext(\"local\", \"Lab2\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: You MUST wait for a cell to complete before you 'run\" the next cell in PySpark Jupyter notebook. Otherwise, the execution of your later cells may generate errors because the inputs they need are not available yet (e.g., have not been generated by a previous PySpark statement that is still running)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (5 points) (a) Add your name below AND (b) replace the path below with the path of your home directory (i.e., replace juy1 with your PSU Access ID).\n",
    "## Answer for Exercise 1\n",
    "- a: Your Name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_RDD = sc.textFile(\"/storage/home/???/work/Lab2/StayingSafeAtPennStateFall2024.txt\")\n",
    "text_RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Parsing Input Text into Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD\n",
    "- RDD is the primary distributed data structure used by Spark for storing/accessing big data in a cluster.  \n",
    "- We will talk more about RDD next week. For now, we will view RDD as a big table.\n",
    "- When we use `textFile` of Spark to read an input file, it returns an RDD (a big table), where each entry in the big table is a line of the input file.\n",
    "- The contents of RDD can be obtained using `.take()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take\n",
    "- Take is an action, which we will discuss more next week.\n",
    "- Applying `take` method on an RDD shows the content of the RDD.  \n",
    "- The parameter of take is the number of entries in the RDD to be accessed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 (5%)\n",
    "Complete the code below to show the first five lines of the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_RDD.take(??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map\n",
    "- Map of Spark applies a function to an input RDD. We often use a `lambda` expression to describe an unnamed function as the parameter for map. \n",
    "- The body of the lambda expression below uses Python `strip` and `split` method (for string).\n",
    "- The Python `strip` method for strings removes spaces at the beginning and at the end of a string.\n",
    "- The Python `split` method for strings split the string using the specified delimiter (which is space `\" \"` in this case).\n",
    "- Returns an `RDD` (a \"big table\") where each row is the result of applying the map function to a corresponding row of the input RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_tokens_RDD = text_RDD.map(lambda line: line.strip().split(\" \"))\n",
    "line_tokens_RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 (5%)\n",
    "Complete the code below to show the first ten entries of the line_tokens_RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_tokens_RDD.???(???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "- Tokens/words in each entry in the text_RDD (i.e., each line in the input text file) are splitted/separated by the space character.\n",
    "- The `lambda` function returns a list of tokens for each line of the text_RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# flatMap \n",
    "The Spark `flatMap` method for RDD returns an RDD that removes the boundary between entries/rows of the input RDD big table.\n",
    "- Applying `flatMap` to `line_tokens_RDD` removes the boundary of different lines in the input text.  In another word, it merges the list of tokens for each lines in the text into a gigantic list of tokens for the entire input document.\n",
    "- Intuively, the effect of `flatMap` can be understood as **flattening** the internal structures of its input RDD.\n",
    "\n",
    "## Results of flatMap:\n",
    "We no longer see the list of tokens in `line_tokens_RDD` the reflects the line structure of the input file.  Instead, each token is an element of this gigantic list: `tokens_RDD`, which we will use to calculate the total number of occurance for each word/token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_RDD = line_tokens_RDD.flatMap(lambda x: x)\n",
    "tokens_RDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 (10%)\n",
    "- (a) Describe, based on the cotents of tokens_RDD shown by the previous statement, the effect of flatMap. (5%)\n",
    "- (b) What is the key difference between the output of map and flatmap ?(5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to Exercise 4:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ttx7g16JsImQ"
   },
   "source": [
    "# Part B: Counting Word Frequency using MapReduce\n",
    "\n",
    "### We want to count the total number of time a word/token occurs in the twitter dataset. We can use the concept of MapReduce to do this in a \"scalable\" way such that we can do this calculation even if the size of twitter dataset is too large to fit into a computer.\n",
    "\n",
    "### A MapReduce way to achieve this involves two steps:\n",
    "### Step 1: map each word into a key value pair \n",
    "`(<word>, 1)`\n",
    "### The key of this key-value pair is the word (in the input RDD); the value of the key-value pair is the number 1.\n",
    "### Step 2: Use reduceByKey in Spark to aggregate all pairs of the same key into\n",
    "`(<word>, <count>)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Lambda function\n",
    "The lambda function that is the parameter of `map` is applied to each element of the input RDD (which is each token in the tokens_RDD). Lambda function (in Python) has the format of\n",
    "`lambda <parameter list>: <function body>`\n",
    "The value returned by the lambda function is either \n",
    "- an explicit `return` statement, or\n",
    "- the value returned by the last statement in the body of the function.\n",
    "\n",
    "In the example below, the lambda function returns the key value pair `(<input word>, 1)`\n",
    "because it is the only (hence also the last) statement in the body of the lambda\n",
    "expression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_1_RDD = tokens_RDD.map(lambda x: (x, 1))\n",
    "token_1_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_1_RDD.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reduceByKey\n",
    "\n",
    "The transformation `reduceByKey` also takes a lambda function. However, this function takes TWO parameters.\n",
    "- The first parameter `lambda x,y: x+y` is an aggregation function, which has two parameters (x and y in the code below).\n",
    "- The first parameter of reduceByKey x is a counter (initial value is 0).\n",
    "- The second parameter of reduceByKey y is the 'value' in input key-value pairs to be aggregated with the counter.  \n",
    "- Since the 'values' in the input key-value pairs are all 1, this lambda function simplies increment the accumulator for each occurance of the key.  Because reduceByKey aggregates across the entire input RDD, the final value (i.e., accumulator) for each word/token is the total number it occurs in the Twitter dataset.\n",
    "- The second parameter is the number of partitions used to partition the keys (so that the reduce task can be distributed among multiple reduce workers for scalability). In the cluster mode, the choice of this number should include considerations about the total number of nodes available in the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 (10 points) Complete the code below by choosing a name for the RDD to be generated by `reduceByKey`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??? = token_1_RDD.reduceByKey(lambda x,y: x+y)\n",
    "???.take(??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 (10 points) Complete the code below by filling in the path of your Tweets word count output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saveAsTextFile\n",
    "This action saves the content of the input RDD in the directory specified.  Even though we add \".txt\" extension to the directory (to remind ourself it contains text file, not CSV),\n",
    "the path is used as a directory to store all partitions of the given RDD (i.e., `token_count_RDD` in this lab). \n",
    "\n",
    "## An important requirement for saveAsTextFile is the path for the output files SHOULD NOT EXIST.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Tip #1: \n",
    "Before you run saveAsTextFile each time, double check whether the output path used does not already exist.  If it does, either \n",
    "- (a) remove the directory, or\n",
    "- (b) change the output path to one that has not been used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"/storage/home/???/work/Lab2/Lab2_output.txt\"\n",
    "WF_RDD.saveAsTextFile(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7 (5 points) Modify the path of input file to create a typo in the INPUT FILE NAME.  Execute the code below. \n",
    "- You do not need to fix the typo.\n",
    "- (a) When do you expect to see the first error message before running the code below?\n",
    "- (b) When does the error message actually show up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2_RDD = sc.textFile(\"???\")\n",
    "text2_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line2_RDD = text2_RDD.map(lambda line: line.strip().split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2_RDD = line2_RDD.flatMap(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2_RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2_1_RDD = token2_RDD.map(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2_count_RDD = token2_1_RDD.reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer to Exercise 7 (10 points):\n",
    "### Type your answer to Exercise 7 below:\n",
    "- (a) \n",
    "- (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `.stop()` method terminates the given SparkContext. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
